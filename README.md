# ğŸ§  Simple-Neural-Network_Basic-Pytorch

This project demonstrates a **basic neural network built using PyTorch** and trained to classify the **MNIST handwritten digits dataset** âœï¸.  
It helped me understand the **end-to-end neural network pipeline** and build a strong foundation in **deep learning** ğŸš€.

---

## ğŸ“Œ Project Overview

This project is my **first neural network implementation** and serves as a **proof of concept for mastering the PyTorch workflow**.  
It covers the complete lifecycle of a neural network, including:

- ğŸ“¥ Data loading  
- ğŸ—ï¸ Model architecture design  
- ğŸ” Training loops  
- ğŸ“Š Model evaluation  

---

## ğŸ“‚ Dataset

**MNIST**  
- ğŸ”¢ Handwritten digits (**0â€“9**)  
- ğŸ–¼ï¸ **Data Type:** Grayscale images  
- ğŸ“ **Image Size:** \(28 \times 28\) pixels  
- ğŸ§® **Input Features:** 784 (when flattened)

---

## ğŸ› ï¸ Tools & Technologies

- ğŸ”¥ **PyTorch** â€“ Core framework for building and training the neural network  
- ğŸ§° **Torchvision** â€“ Dataset downloading and image transformations  
- ğŸ **Python** â€“ Base programming language  

---

## ğŸš§ Progress Involved

### 1ï¸âƒ£ Loading the Data
- Imported the MNIST dataset
- Applied image transformations:
  - `ToTensor()` to convert images into tensors
- Split the dataset into:
  - ğŸ‹ï¸ Training set
  - ğŸ§ª Testing set
- Used `DataLoader` for:
  - Efficient batching
  - Data shuffling

---

### 2ï¸âƒ£ Building the Model
- Created a custom model class inheriting from `nn.Module`
- **Layers:**
  - Used `nn.Linear` layers for feature processing
- **Activation Function:**
  - Integrated **ReLU (Rectified Linear Unit)** âš¡ for non-linearity
- **Forward Function:**
  - Defined how data flows through the network to generate predictions

---

### 3ï¸âƒ£ Defining Training Logic
- **Loss Function (Criterion):**
  - Used `CrossEntropyLoss` ğŸ“‰ to measure prediction error
- **Optimizer:**
  - Used optimization algorithms such as:
    - SGD
    - Adam âš™ï¸  
  - Responsible for updating model weights

---

### 4ï¸âƒ£ Training the Model
- Trained the model for a fixed number of epochs (`n_epochs`)
- For each batch:
  1. ğŸ”® Forward pass (prediction)
  2. ğŸ“‰ Loss computation
  3. ğŸ” Backpropagation using `loss.backward()`
  4. ğŸ”§ Weight updates using the optimizer

---

### 5ï¸âƒ£ Testing & Evaluation
- Loaded the test dataset
- Passed test images through the trained model
- Compared predicted labels with true labels
- Calculated final **accuracy percentage** ğŸ“Š

---

### 6ï¸âƒ£ Results ğŸ†
- âœ… **Accuracy on 10,000 test images:** **97.14%**

---

## ğŸ“š Key Learnings

- ğŸ”„ **Data Pipeline:**  
  Transforming and batching raw image data effectively
- ğŸ—ï¸ **Model Architecture:**  
  Building structured models using `nn.Module`, linear layers, and activations
- ğŸ“Š **Evaluation Process:**  
  Understanding the difference between:
  - Training phase (with gradient computation)
  - Testing phase (evaluation without gradients)

---
